{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import time\n",
    "import os.path\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoder, SQLTransformer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat,translate,lit,col,isnan,count,when,split,explode,ltrim,create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise spark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName='Classifier')\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframes(directory):\n",
    "    \"\"\"\n",
    "    Creates dataframes from directory\n",
    "    Must be named 'train' or 'test'. \n",
    "    Returns only train if test N/A\n",
    "    \n",
    "    Inputs: String \n",
    "    \n",
    "    Returns: Dataframes/Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(directory):\n",
    "        train = directory+\"/train.csv\"\n",
    "        if os.path.exists(train):\n",
    "            df_train = sql.read.csv(train, \n",
    "                         header = True,\n",
    "                         inferSchema = True)\n",
    "        else:\n",
    "            raise ValueError(\"train.csv not found in %s\" % directory)\n",
    "        \n",
    "        test = directory+\"/test.csv\"\n",
    "        if os.path.exists(test):\n",
    "            df_test = sql.read.csv(train, \n",
    "                         header = True,\n",
    "                         inferSchema = True)\n",
    "            return df_train,df_test\n",
    "        \n",
    "        return df_train\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"%s does not exist\" % directory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train,df_test= create_dataframes('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine train and test\n",
    "df_train = df_train.withColumn('Mark',lit('train'))\n",
    "df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "df = df_train.unionAll(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for PassengerId : 0\n",
      "Missing values for Survived : 0\n",
      "Missing values for Pclass : 0\n",
      "Missing values for Name : 0\n",
      "Missing values for Sex : 0\n",
      "Missing values for Age : 0\n",
      "Missing values for SibSp : 0\n",
      "Missing values for Parch : 0\n",
      "Missing values for Ticket : 0\n",
      "Missing values for Fare : 0\n",
      "Missing values for Embarked : 0\n",
      "Missing values for Mark : 0\n"
     ]
    }
   ],
   "source": [
    "#missing values by column\n",
    "for column in df.columns:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cabin has a high amount of missing values so I will remove it \n",
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values with the mean\n",
    "def fill_null_with_mean(df):\n",
    "    \"\"\"\n",
    "    Replaces null numeric values with\n",
    "    mean value\n",
    "    Replaces categorical string values\n",
    "    with mode\n",
    "    input: spark dataframe\n",
    "    returns: spark dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        dtype = \"%s\" % column.dataType\n",
    "        if dtype != \"StringType\":\n",
    "            mean = df.groupBy().mean(column.name).first()[0]\n",
    "            x = x.na.fill({column.name:mean})\n",
    "        else:\n",
    "            counts = df.groupBy(column.name).count()\n",
    "            mode = counts.join(\n",
    "            counts.agg(F.max(\"count\").alias(\"max_\")),\n",
    "            col(\"count\") == col(\"max_\")\n",
    "            ).limit(1).select(column.name)\n",
    "            x = x.na.fill({column.name:mode.first()[0]})     \n",
    "    return x\n",
    "\n",
    "df = fill_null_with_mean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title cleanse \n",
    "df = df.withColumn('Surname',split('Name',', ')[0])\n",
    "df = df.withColumn('name_split',split('Name',', ')[1])\n",
    "df = df.withColumn('Title',ltrim(split('name_split','. ')[0]))\n",
    "df = df.drop('name_split')\n",
    "title_dictionary = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Sir\",\n",
    "    \"Don\":        \"Sir\",\n",
    "    \"Sir\" :       \"Sir\",\n",
    "    \"Dr\":         \"Mr\",\n",
    "    \"Rev\":        \"Mr\",\n",
    "    \"the Countess\":\"Lady\",\n",
    "    \"Dona\":       \"Lady\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Lady\"\n",
    "}\n",
    "\n",
    "#x = df['Title'].map(Title_Dictionary)\n",
    "mapping_expr = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "\n",
    "df = df.withColumn(\"Title\", mapping_expr.getItem(col(\"Title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary column 'Mother'\n",
    "df = df.withColumn('Mother',when((df['Sex'] =='female')&\n",
    "                                (df['Age'] > 18)&\n",
    "                                (df['Parch'] > 0)\n",
    "                                 ,'Mother').otherwise('null'))\n",
    "\n",
    "#create a family size column\n",
    "df = df.withColumn('Family_size',(df['SibSp'] + df['Parch'] + 1))\n",
    "\n",
    "# create a family id column\n",
    "df = df.withColumn('Family_id',when(df['Family_size']>2,\n",
    "                                   (concat(df['Surname'],\n",
    "                                    df['Family_size']))).otherwise('null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+---------+\n",
      "| Title|Mother|Family_size|Family_id|\n",
      "+------+------+-----------+---------+\n",
      "|    Mr|  null|          2|     null|\n",
      "|   Mrs|  null|          2|     null|\n",
      "|  Miss|  null|          1|     null|\n",
      "|   Mrs|  null|          2|     null|\n",
      "|    Mr|  null|          1|     null|\n",
      "|    Mr|  null|          1|     null|\n",
      "|    Mr|  null|          1|     null|\n",
      "|Master|  null|          5| Palsson5|\n",
      "|   Mrs|Mother|          3| Johnson3|\n",
      "|   Mrs|  null|          2|     null|\n",
      "+------+------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Title','Mother','Family_size','Family_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns \n",
    "df = df.drop('PassengerId','Ticket','Surname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Mark', 'Title', 'Mother', 'Family_size', 'Family_id']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_column_types(df):\n",
    "    \"\"\"\n",
    "    Create array of numeric and string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    string = []\n",
    "    numeric = []\n",
    "    \n",
    "    for col in df.schema.fields:\n",
    "        x = \"%s\" % col.dataType\n",
    "        if x == \"StringType\":\n",
    "            string.append(col.name)\n",
    "        else:\n",
    "            numeric.append(col.name)\n",
    "            \n",
    "            \n",
    "    return string,numeric\n",
    "\n",
    "categorical,numeric = split_on_column_types(df)\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_414aa69545b37ad78400, StringIndexer_4c0094e49443a1d9fe42, StringIndexer_46c28b2da26c0ba06928, StringIndexer_4295adafd12a1dcf5516, StringIndexer_414f9e1760e230db70f8, StringIndexer_4fd8b8d9566eeb3a9faf, StringIndexer_45e3bc2a297e0a3726ab, OneHotEncoder_4cf18fc8f3eb1288e92e, OneHotEncoder_4f21a1db5489811826fa, OneHotEncoder_461ba57f94e3fd7a0cbf, OneHotEncoder_40a1b9c5161d8d1dc48f, OneHotEncoder_4eb9858eafe645703a53, OneHotEncoder_409e98159d523315de7e, OneHotEncoder_42dab6cb7adefbf0f5fc]\n"
     ]
    }
   ],
   "source": [
    "indexers.extend(encoders)\n",
    "print(indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column Survived must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually IntegerType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3952.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column Survived must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually IntegerType.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\r\n\tat org.apache.spark.ml.feature.StandardScalerParams$class.validateAndTransformSchema(StandardScaler.scala:67)\r\n\tat org.apache.spark.ml.feature.StandardScaler.validateAndTransformSchema(StandardScaler.scala:87)\r\n\tat org.apache.spark.ml.feature.StandardScaler.transformSchema(StandardScaler.scala:123)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:112)\r\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:87)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-e8b9313bed0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n\u001b[0;32m      4\u001b[0m                          \u001b[1;33m,\u001b[0m\u001b[0mwithStd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwithMean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                          ).fit(df) for column in numeric]\n\u001b[0m",
      "\u001b[1;32m<ipython-input-201-e8b9313bed0b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n\u001b[0;32m      4\u001b[0m                          \u001b[1;33m,\u001b[0m\u001b[0mwithStd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwithMean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                          ).fit(df) for column in numeric]\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column Survived must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually IntegerType.'"
     ]
    }
   ],
   "source": [
    "#scale numeric columns\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n",
    "                         ,withStd=False,withMean=False\n",
    "                         ).fit(df) for column in numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+----+-----+-----+-------+--------+-----+-----+------+-----------+---------+\n",
      "|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Fare|Embarked| Mark|Title|Mother|Family_size|Family_id|\n",
      "+--------+------+--------------------+------+----+-----+-----+-------+--------+-----+-----+------+-----------+---------+\n",
      "|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|   7.25|       S|train|   Mr|  null|          2|     null|\n",
      "|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|71.2833|       C|train|  Mrs|  null|          2|     null|\n",
      "+--------+------+--------------------+------+----+-----+-----+-------+--------+-----+-----+------+-----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "df_vec=reduce(DataFrame.drop,categorical,df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+----------+---------+--------------+----------+-----------+------------+---------------+-----------------+-------------+-------------+-------------+-------------+-------------+--------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_size|Name_index|Sex_index|Embarked_index|Mark_index|Title_index|Mother_index|Family_id_index|         Name_vec|      Sex_vec| Embarked_vec|     Mark_vec|    Title_vec|   Mother_vec| Family_id_vec|\n",
      "+--------+------+----+-----+-----+-------+-----------+----------+---------+--------------+----------+-----------+------------+---------------+-----------------+-------------+-------------+-------------+-------------+-------------+--------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          2|     329.0|      0.0|           0.0|       0.0|        0.0|         0.0|            0.0|(890,[329],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(1,[0],[1.0])|(6,[0],[1.0])|(1,[0],[1.0])|(95,[0],[1.0])|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          2|     805.0|      1.0|           1.0|       0.0|        2.0|         0.0|            0.0|(890,[805],[1.0])|    (1,[],[])|(2,[1],[1.0])|(1,[0],[1.0])|(6,[2],[1.0])|(1,[0],[1.0])|(95,[0],[1.0])|\n",
      "+--------+------+----+-----+-----+-------+-----------+----------+---------+--------------+----------+-----------+------------+---------------+-----------------+-------------+-------------+-------------+-------------+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vec.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
