{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import time\n",
    "import os.path\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString, VectorIndexer, VectorAssembler, OneHotEncoder, SQLTransformer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import StructType,StringType,StructField,IntegerType,DoubleType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat,translate,lit,col,isnan,count,when,split,explode,ltrim,create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise spark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName='Classifier')\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(directory,schema_train=None,schema_test=None):\n",
    "    \"\"\"\n",
    "    Creates dataframes from directory\n",
    "    Must be named 'train' or 'test'. \n",
    "    Returns only train if test N/A\n",
    "    \n",
    "    Inputs: String, schema defaults to false\n",
    "    and will infer from input .csv else will apply\n",
    "    specified schema/schemas\n",
    "    \n",
    "    Returns: Dataframes/Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    inferSchema = True if schema==None else False\n",
    "    if schema_test==None:\n",
    "        schema_test = schema\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        train_path = directory+\"/train.csv\"\n",
    "        if os.path.exists(train_path):\n",
    "            df_train = sql.read.csv(train_path, \n",
    "                         header = True,\n",
    "                         inferSchema = True)\n",
    "        else:\n",
    "            raise ValueError(\"train.csv not found in %s\" % directory)\n",
    "        \n",
    "        test_path = directory+\"/test.csv\"\n",
    "        if os.path.exists(test_path):\n",
    "            df_test = sql.read.csv(test_path, \n",
    "                         header = True,\n",
    "                         inferSchema=True)\n",
    "            \n",
    "            return df_train,df_test\n",
    "        \n",
    "        return df_train\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"%s does not exist\" % directory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path = \"./data/test.csv\"\n",
    "# if os.path.exists(test_path):\n",
    "#     df_test = sql.read.csv(test_path, \n",
    "#                          header = True,\n",
    "#                          inferSchema = True)\n",
    "# print(df_test.schema.fields)\n",
    "\n",
    "# df_train = sql.read.csv(test_path, \n",
    "#                          header = True,\n",
    "#                          inferSchema = True,\n",
    "#                          schema=None)\n",
    "\n",
    "# df_train.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify schema\n",
    "# schema= StructType([\n",
    "#     StructField(\"PassengerId\",IntegerType(),True),\n",
    "#     StructField(\"Survived\",StringType(),True),\n",
    "#     StructField(\"Pclass\",StringType(),True),\n",
    "#     StructField(\"Name\",StringType(),True),\n",
    "#     StructField(\"Sex\",StringType(),True),\n",
    "#     StructField(\"Age\",DoubleType(),True),\n",
    "#     StructField(\"SibSp\",DoubleType(),True),\n",
    "#     StructField(\"Parch\",DoubleType(),True),\n",
    "#     StructField(\"Ticket\",StringType(),True),\n",
    "#     StructField(\"Fare\",DoubleType(),True),\n",
    "#     StructField(\"Cabin\",StringType(),True),\n",
    "#     StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "# schema_test= StructType([\n",
    "#     StructField(\"PassengerId\",IntegerType(),True),\n",
    "#     StructField(\"Pclass\",StringType(),True),\n",
    "#     StructField(\"Name\",StringType(),True),\n",
    "#     StructField(\"Sex\",StringType(),True),\n",
    "#     StructField(\"Age\",DoubleType(),True),\n",
    "#     StructField(\"SibSp\",DoubleType(),True),\n",
    "#     StructField(\"Parch\",DoubleType(),True),\n",
    "#     StructField(\"Ticket\",StringType(),True),\n",
    "#     StructField(\"Fare\",DoubleType(),True),\n",
    "#     StructField(\"Cabin\",StringType(),True),\n",
    "#     StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "\n",
    "\n",
    "df_train,df_test=create_dataframes('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine train and test\n",
    "# df_train = df_train.withColumn('Mark',lit('train'))\n",
    "# df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "# #create a blank 'Survived' column in df_test for join\n",
    "# df_test = df_test.withColumn('Survived',lit('unknown'))\n",
    "# #rearrange columns to avoid mis label when grouping together\n",
    "# df_test = df_test.select(df_train.columns)\n",
    "# df = df_train.union(df_test)\n",
    "# #When the table is joined Fare is converted to a string so convert back to double\n",
    "# df= df.withColumn(\"Fare\", df[\"Fare\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_train_test(df_train,df_test,label):\n",
    "    \"\"\"\n",
    "    Combine train and test dataframes\n",
    "    Creates dummy colum if label not in test\n",
    "    \n",
    "    inputs: 2 DataFrames\n",
    "    \n",
    "    returns: DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    #Mark dataframes\n",
    "    df_train = df_train.withColumn('Mark',lit('train'))\n",
    "    df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "    \n",
    "    def has_column(df, col):\n",
    "        try:\n",
    "            df[col]\n",
    "            return True\n",
    "        except AnalysisException:\n",
    "            return False\n",
    "    \n",
    "    if has_column(df_test,label):\n",
    "        if len(df_train.columns) == len(df_test.columns):\n",
    "            #rearrange columns to avoid mis label when grouping together\n",
    "            df_test = df_test.select(df_train.columns)\n",
    "            return (df_train.union(df_test))\n",
    "        else:\n",
    "            raise ValueError(\"input dataframes of different shape\")\n",
    "    else:\n",
    "        #add dummy label column to dataframe\n",
    "        df_test = df_test.withColumn(label,lit('unknown'))\n",
    "        if len(df_train.columns) == len(df_test.columns):\n",
    "            df_test = df_test.select(df_train.columns)\n",
    "            return (df_train.union(df_test))\n",
    "        else:\n",
    "            raise ValueError(\"input dataframes of different shape\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_train_test(df_train,df_test,'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#missing values by column\n",
    "def get_missing(df):\n",
    "    for column in df.columns:\n",
    "        missing = df.where(df[column].isNull()).count()\n",
    "        print(\"Missing values for %s : %s\" % (column,missing))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cabin has a high amount of missing values so I will remove it \n",
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "Survived\n",
      "Pclass\n",
      "Name\n",
      "Sex\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Ticket\n",
      "Fare\n",
      "Embarked\n",
      "Mark\n"
     ]
    }
   ],
   "source": [
    "#fill missing values with the mean\n",
    "def fill_null_with_mean(df):\n",
    "    \"\"\"\n",
    "    Replaces null numeric values with\n",
    "    mean value\n",
    "    Replaces categorical string values\n",
    "    with mode\n",
    "    input: spark dataframe\n",
    "    returns: spark dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        print(column.name)\n",
    "        if df.where(df[column.name].isNull()).count() > 0:\n",
    "            \n",
    "            dtype = \"%s\" % column.dataType\n",
    "            if dtype != \"StringType\":\n",
    "                mean = df.groupBy().mean(column.name).first()[0]\n",
    "                x = x.na.fill({column.name:mean})\n",
    "            else:\n",
    "                counts = df.groupBy(column.name).count()\n",
    "                mode = counts.join(\n",
    "                counts.agg(F.max(\"count\").alias(\"max_\")),\n",
    "                col(\"count\") == col(\"max_\")\n",
    "                ).limit(1).select(column.name)\n",
    "                x = x.na.fill({column.name:mode.first()[0]})     \n",
    "    return x\n",
    "\n",
    "df = fill_null_with_mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning method above could be much improved to replace missing values than with the mean but for this notebook I wanted something quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove spaces\n",
    "spaceDeleteUDF = F.udf(lambda s: s.replace(\" \", \"\"),StringType())\n",
    "df=df.withColumn('Name',spaceDeleteUDF(df[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title cleanse \n",
    "df = df.withColumn('Surname',F.trim(split('Name',',')[0]))\n",
    "df = df.withColumn('name_split',F.trim(split('Name',',')[1]))\n",
    "df = df.withColumn('Title',F.trim(split('name_split','\\\\.')[0]))\n",
    "title_dictionary = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Sir\",\n",
    "    \"Don\":        \"Sir\",\n",
    "    \"Sir\" :       \"Sir\",\n",
    "    \"Dr\":         \"Mr\",\n",
    "    \"Rev\":        \"Mr\",\n",
    "    \"theCountess\":\"Lady\",\n",
    "    \"Dona\":       \"Lady\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Lady\"\n",
    "}\n",
    "\n",
    "#x = df['Title'].map(Title_Dictionary)\n",
    "mapping_expr = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "\n",
    "df = df.withColumn(\"Title\", mapping_expr.getItem(col(\"Title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create binary column 'Mother'\n",
    "df = df.withColumn('Mother',when((df['Sex'] =='female')&\n",
    "                                (df['Age'] > 18)&\n",
    "                                (df['Parch'] > 0)\n",
    "                                 ,'True').otherwise('False'))\n",
    "\n",
    "#create a family size column\n",
    "df = df.withColumn('Family_size',(df['SibSp'] + df['Parch'] + 1))\n",
    "\n",
    "# create a family id column\n",
    "df = df.withColumn('Family_id',when(df['Family_size']>2,\n",
    "                                   (concat(df['Surname'],\n",
    "                                    df['Family_size']))).otherwise('None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for Title : 0\n",
      "Missing values for Mother : 0\n",
      "Missing values for Family_size : 0\n",
      "Missing values for Family_id : 0\n",
      "Missing values for Surname : 0\n"
     ]
    }
   ],
   "source": [
    "for column in ['Title','Mother','Family_size','Family_id','Surname']:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns \n",
    "df = df.drop('Ticket','Surname','Name','name_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_column_types(df):\n",
    "    \"\"\"\n",
    "    Create array of numeric and string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    for col in df.schema.fields:\n",
    "        x = \"%s\" % col.dataType\n",
    "        if x == \"StringType\":\n",
    "            categorical.append(col.name)\n",
    "        else:\n",
    "            numeric.append(col.name)\n",
    "            \n",
    "            \n",
    "    return categorical,numeric\n",
    "\n",
    "#categorical,numeric = split_on_column_types(train)\n",
    "#indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "#encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(df,label):\n",
    "    \"\"\"\n",
    "    Convert dataframe into a features\n",
    "    \n",
    "    \"\"\"\n",
    "    x = df.cache()\n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    \n",
    "    for column in x.schema.fields:\n",
    "        if column.name != label:\n",
    "            cType = \"%s\" % column.dataType\n",
    "            if cType == \"StringType\":\n",
    "                categorical.append(column.name)\n",
    "            else:\n",
    "                numeric.append(column.name)\n",
    "            \n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "    labelIndexer = StringIndexer(inputCol=label,outputCol=label+'_index').fit(x)\n",
    "    #encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]\n",
    "    index_categorical = [column + \"_index\" for column in categorical]\n",
    "    all_columns = index_categorical + numeric\n",
    "    assembler = VectorAssembler(inputCols=all_columns,outputCol='features')\n",
    "    #assembler is added to list with square brackets\n",
    "    stages = indexers + [labelIndexer,assembler]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    x = pipeline.fit(x).transform(x)\n",
    "    return x\n",
    "\n",
    "convert_test = convert(df,'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(13,[6,7,8,9,11,1...|\n",
      "|[1.0,1.0,0.0,2.0,...|\n",
      "|(13,[0,3,6,7,8,11...|\n",
      "|[1.0,0.0,0.0,2.0,...|\n",
      "|(13,[6,7,8,11,12]...|\n",
      "|(13,[1,6,7,8,11,1...|\n",
      "|(13,[6,7,8,11,12]...|\n",
      "|[0.0,0.0,0.0,3.0,...|\n",
      "|[1.0,0.0,0.0,2.0,...|\n",
      "|[1.0,1.0,0.0,2.0,...|\n",
      "|[1.0,0.0,0.0,1.0,...|\n",
      "|(13,[0,3,6,7,8,11...|\n",
      "|(13,[6,7,8,11,12]...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|(13,[0,3,6,7,8,11...|\n",
      "|(13,[0,3,6,7,8,11...|\n",
      "|[0.0,2.0,0.0,3.0,...|\n",
      "|(13,[6,7,8,11,12]...|\n",
      "|[1.0,0.0,0.0,2.0,...|\n",
      "|[1.0,1.0,0.0,2.0,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_test.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split back into train and test \n",
    "train = df.where(df['Mark']=='train')\n",
    "test  = df.where(df['Mark']=='test')\n",
    "\n",
    "#append 'Survived' back on training data\n",
    "# since there is no common column between these two dataframes add row_index so that it can be joined\n",
    "train=train.withColumn('row_index', F.monotonically_increasing_id())\n",
    "survived = df_train.select('Survived')\n",
    "labelIndexer = StringIndexer(inputCol='Survived',outputCol='Survived_index').fit(survived)\n",
    "survived = labelIndexer.transform(survived)\n",
    "survived = survived.withColumn('row_index', F.monotonically_increasing_id())\n",
    "train = train.join(survived, on=[\"row_index\"]).sort(\"row_index\").drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Survived_index\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=100,\n",
    "                            maxBins=107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_fit = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = rf_fit.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol='prediction',\n",
    "                               outputCol='predictedLabel',\n",
    "                               labels=labelIndexer.labels)\n",
    "pred = labelConverter.transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred.withColumn(\"Survived\", pred[\"prediction\"]).select(\"PassengerId\", \"Survived\")\n",
    "predictions.write.format('com.databricks.spark.csv').save('./data/predict6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in predictions.columns:\n",
    "    missing = predictions.where(predictions[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.coalesce(1).write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #scale numeric columns\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n",
    "#                          ,withStd=False,withMean=False\n",
    "#                          ).fit(df) for column in numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
