{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import time\n",
    "import os.path\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler,StringIndexer,IndexToString, VectorIndexer, VectorAssembler, OneHotEncoder, SQLTransformer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import StructType,StringType,StructField,IntegerType,DoubleType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat,translate,lit,col,isnan,count,when,split,explode,ltrim,create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise spark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName='Classifier')\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframes(directory,schema_train=None,schema_test=None):\n",
    "    \"\"\"\n",
    "    Creates dataframes from directory\n",
    "    Must be named 'train' or 'test'. \n",
    "    Returns only train if test N/A\n",
    "    \n",
    "    Inputs: String, schema defaults to false\n",
    "    and will infer from input .csv else will apply\n",
    "    specified schema/schemas\n",
    "    \n",
    "    Returns: Dataframes/Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    inferSchema = True if schema_train==None else False\n",
    "    if schema_test==None:\n",
    "        schema_test = schema_train\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        train_path = directory+\"/train.csv\"\n",
    "        if os.path.exists(train_path):\n",
    "            df_train = sql.read.csv(train_path, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema_train)\n",
    "        else:\n",
    "            raise ValueError(\"train.csv not found in %s\" % directory)\n",
    "        \n",
    "        test_path = directory+\"/test.csv\"\n",
    "        if os.path.exists(test_path):\n",
    "            df_test = sql.read.csv(test_path, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema_test)\n",
    "            \n",
    "            return df_train,df_test\n",
    "        \n",
    "        return df_train\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"%s does not exist\" % directory)   \n",
    "        \n",
    "df_train,df_test=create_dataframes('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_train_test(df_train,df_test,label):\n",
    "    \"\"\"\n",
    "    Combine train and test dataframes\n",
    "    Creates dummy colum if label not in test\n",
    "    \n",
    "    inputs: 2 DataFrames\n",
    "    \n",
    "    returns: DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    #Mark dataframes\n",
    "    df_train = df_train.withColumn('Mark',lit('train'))\n",
    "    df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "    \n",
    "    def has_column(df, column):\n",
    "        try:\n",
    "            df[column]\n",
    "            return True\n",
    "        except AnalysisException:\n",
    "            return False\n",
    "    \n",
    "    if has_column(df_test,label):\n",
    "        if len(df_train.columns) == len(df_test.columns):\n",
    "            #rearrange columns to avoid mis label when grouping together\n",
    "            df_test = df_test.select(df_train.columns)\n",
    "            return (df_train.union(df_test))\n",
    "        else:\n",
    "            raise ValueError(\"input dataframes of different shape\")\n",
    "    else:\n",
    "        #add dummy label column to dataframe\n",
    "        df_test = df_test.withColumn(label,lit(0))\n",
    "        if len(df_train.columns) == len(df_test.columns):\n",
    "            df_test = df_test.select(df_train.columns)\n",
    "            return (df_train.union(df_test))\n",
    "        else:\n",
    "            raise ValueError(\"input dataframes of different shape\")\n",
    "            \n",
    "df = combine_train_test(df_train,df_test,'Survived')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#missing values by column\n",
    "def get_missing(df):\n",
    "    \"\"\"\n",
    "    Prints no. missing values for each column\n",
    "    \n",
    "    inputs: df - Spark DataFrame\n",
    "    \n",
    "    returns: None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing = df.where(df[column].isNull()).count()\n",
    "        print(\"Missing values for %s : %s\" % (column,missing))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_missing_columns(df,thresh=0.05,ignore=[]):\n",
    "    \"\"\"\n",
    "    Removes column from dataframe if the column\n",
    "    has higher number of null values than thresh \n",
    "    \n",
    "    \n",
    "    inputs: DataFrame, float - thresh (defaults to 0.05),\n",
    "    ignore - Array (list of columns to be exempt)\n",
    "    \n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    columns = filter(lambda x: x not in ignore,x.columns)\n",
    "    \n",
    "    for column in columns:\n",
    "        missing = df.where(df[column].isNull()).count()\n",
    "        if missing != 0:\n",
    "            if (missing/x.count()) > thresh:\n",
    "                x=x.drop(column)\n",
    "    \n",
    "    return x\n",
    " \n",
    "df = remove_missing_columns(df,thresh=0.50,ignore=['Age','Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill missing values with the mean\n",
    "def fill_null_with_mean(df):\n",
    "    \"\"\"\n",
    "    Replaces null numeric values with\n",
    "    mean value\n",
    "    Replaces categorical string values\n",
    "    with mode\n",
    "    input: spark dataframe\n",
    "    returns: spark dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        if df.where(df[column.name].isNull()).count() > 0:\n",
    "            \n",
    "            dtype = \"%s\" % column.dataType\n",
    "            if dtype != \"StringType\":\n",
    "                mean = df.groupBy().mean(column.name).first()[0]\n",
    "                x = x.na.fill({column.name:mean})\n",
    "            else:\n",
    "                counts = df.groupBy(column.name).count()\n",
    "                mode = counts.join(\n",
    "                counts.agg(F.max(\"count\").alias(\"max_\")),\n",
    "                col(\"count\") == col(\"max_\")\n",
    "                ).limit(1).select(column.name)\n",
    "                x = x.na.fill({column.name:mode.first()[0]})     \n",
    "    return x\n",
    "\n",
    "df = fill_null_with_mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning method above could be much improved to replace missing values than with the mean but for this notebook I wanted something quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove spaces\n",
    "spaceDeleteUDF = F.udf(lambda s: s.replace(\" \", \"\"),StringType())\n",
    "df=df.withColumn('Name',spaceDeleteUDF(df[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Title cleanse \n",
    "df = df.withColumn('Surname',F.trim(split('Name',',')[0]))\n",
    "df = df.withColumn('name_split',F.trim(split('Name',',')[1]))\n",
    "df = df.withColumn('Title',F.trim(split('name_split','\\\\.')[0]))\n",
    "title_dictionary = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Sir\",\n",
    "    \"Don\":        \"Sir\",\n",
    "    \"Sir\" :       \"Sir\",\n",
    "    \"Dr\":         \"Mr\",\n",
    "    \"Rev\":        \"Mr\",\n",
    "    \"theCountess\":\"Lady\",\n",
    "    \"Dona\":       \"Lady\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Lady\"\n",
    "}\n",
    "\n",
    "#x = df['Title'].map(Title_Dictionary)\n",
    "mapping_expr = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "\n",
    "df = df.withColumn(\"Title\", mapping_expr.getItem(col(\"Title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create binary column 'Mother'\n",
    "df = df.withColumn('Mother',when((df['Sex'] =='female')&\n",
    "                                (df['Age'] > 18)&\n",
    "                                (df['Parch'] > 0)\n",
    "                                 ,'True').otherwise('False'))\n",
    "\n",
    "#create a family size column\n",
    "df = df.withColumn('Family_size',(df['SibSp'] + df['Parch'] + 1))\n",
    "\n",
    "# create a family id column\n",
    "df = df.withColumn('Family',when((df['Family_size']>2),\n",
    "                                    'Family').otherwise('No_Family'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns \n",
    "df = df.drop('Ticket','Surname','Name','name_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_column_types(df):\n",
    "    \"\"\"\n",
    "    Create array of numeric and string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    for col in df.schema.fields:\n",
    "        x = \"%s\" % col.dataType\n",
    "        if x == \"StringType\":\n",
    "            categorical.append(col.name)\n",
    "        else:\n",
    "            numeric.append(col.name)\n",
    "            \n",
    "            \n",
    "    return categorical,numeric\n",
    "\n",
    "#categorical,numeric = split_on_column_types(train)\n",
    "#indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "#encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Survived','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pipeline(df,label):\n",
    "    \"\"\"\n",
    "    Build pipeline to fit and transform data on\n",
    "    \n",
    "    Inputs: df - spark DataFrame, label - String relating label column\n",
    "    \n",
    "    Returns: pipeline and cross validation object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        if column.name != label and column.name !='Mark':\n",
    "            cType = \"%s\" % column.dataType\n",
    "            if cType == \"StringType\":\n",
    "                categorical.append(column.name)\n",
    "            else:\n",
    "                numeric.append(column.name)\n",
    "            \n",
    "    indexers = [StringIndexer(inputCol=column,\n",
    "                              outputCol=column+\"_index\")\\\n",
    "                              for column in categorical]\n",
    "    labelIndexer = StringIndexer(inputCol=label,outputCol=label+\"_index\").fit(df)\n",
    "    index_categorical = [column + \"_index\" for column in categorical]\n",
    "    all_columns = index_categorical + numeric\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=all_columns,outputCol=\"features\")\n",
    "    scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=label+\"_index\",\n",
    "                            featuresCol=\"scaledFeatures\",\n",
    "                            numTrees=10,\n",
    "                            maxBins=200)\n",
    "    \n",
    "    #Used to convert predicted values back to their original format\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                                   outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "    \n",
    "    \n",
    "    #assembler is added to list with square brackets\n",
    "    stages = indexers + [labelIndexer,assembler,scaler,rf,labelConverter]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    \n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "                .addGrid(rf.maxBins,[25,50,75])\\\n",
    "                .addGrid(rf.maxDepth,[4,6,8])\\\n",
    "                .build()\n",
    "                \n",
    "    crossVal = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=BinaryClassificationEvaluator(),\n",
    "                              numFolds=4)\n",
    "    \n",
    "    return pipeline,crossVal\n",
    "\n",
    "pipeline,crossVal = build_pipeline(df,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_train_test(df,train_sample_size=0.7):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into train and test.\n",
    "    If dataframe contains no column 'Mark'\n",
    "    it splits on default 0.7/0.3 random sampling\n",
    "    \n",
    "    inputs: df - Spark DataFrame, train_sample_size - float (0-1)\n",
    "    \n",
    "    returns: train - Spark DataFrame, test - Spark DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    def has_column(df, col):\n",
    "        try:\n",
    "            df[col]\n",
    "            return True\n",
    "        except AnalysisException:\n",
    "            return False\n",
    "    \n",
    "    if has_column(df,'Mark'):\n",
    "        train = df.where(df['Mark']=='train')\n",
    "        train = train.drop('Mark')\n",
    "        test  = df.where(df['Mark']=='test')\n",
    "        test = test.drop('Mark')\n",
    "    \n",
    "    else:\n",
    "        if train_sample_size > 1 or train_sample_size < 0:\n",
    "            raise ValueError(\"train_sample_size out of bounds\")\n",
    "        test_sample_size = 1 - train_sample_size\n",
    "        (train,test) = df.randomSplit([train_sample_size,\n",
    "                                       test_sample_size])\n",
    "        \n",
    "    return train,test\n",
    "        \n",
    "train,test = split_into_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = pipeline.fit(train)\n",
    "#pred = model.transform(test)\n",
    "cvModel = crossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pred.withColumn(\"Survived\", pred[\"predictedLabel\"]).select(\"PassengerId\", \"Survived\")\n",
    "predictions.coalesce(1).write.format('com.databricks.spark.csv') \\\n",
    ".mode('overwrite').option(\"header\", \"true\").save('./data/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #scale numeric columns\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n",
    "#                          ,withStd=False,withMean=False\n",
    "#                          ).fit(df) for column in numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
