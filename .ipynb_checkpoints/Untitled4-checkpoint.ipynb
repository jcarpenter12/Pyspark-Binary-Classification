{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import time\n",
    "import os.path\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString, VectorIndexer, VectorAssembler, OneHotEncoder, SQLTransformer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import StructType,StringType,StructField,IntegerType,DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat,translate,lit,col,isnan,count,when,split,explode,ltrim,create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialise spark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName='Classifier')\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframes(directory,schema_train=None,schema_test=None):\n",
    "    \"\"\"\n",
    "    Creates dataframes from directory\n",
    "    Must be named 'train' or 'test'. \n",
    "    Returns only train if test N/A\n",
    "    \n",
    "    Inputs: String, schema defaults to false\n",
    "    and will infer from input .csv else will apply\n",
    "    specified schema/schemas\n",
    "    \n",
    "    Returns: Dataframes/Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    inferSchema = True if schema==None else False\n",
    "    schema_test = schema_train if schema_test==None else schema_test\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        train = directory+\"/train.csv\"\n",
    "        if os.path.exists(train):\n",
    "            df_train = sql.read.csv(train, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema)\n",
    "        else:\n",
    "            raise ValueError(\"train.csv not found in %s\" % directory)\n",
    "        \n",
    "        test = directory+\"/test.csv\"\n",
    "        if os.path.exists(test):\n",
    "            df_test = sql.read.csv(test, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema_test)\n",
    "            \n",
    "            return df_train,df_test\n",
    "        \n",
    "        return df_train\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"%s does not exist\" % directory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify schema\n",
    "schema= StructType([\n",
    "    StructField(\"PassengerId\",IntegerType(),True),\n",
    "    StructField(\"Survived\",StringType(),True),\n",
    "    StructField(\"Pclass\",StringType(),True),\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sex\",StringType(),True),\n",
    "    StructField(\"Age\",DoubleType(),True),\n",
    "    StructField(\"SibSp\",DoubleType(),True),\n",
    "    StructField(\"Parch\",DoubleType(),True),\n",
    "    StructField(\"Ticket\",StringType(),True),\n",
    "    StructField(\"Fare\",DoubleType(),True),\n",
    "    StructField(\"Cabin\",StringType(),True),\n",
    "    StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "schema_test= StructType([\n",
    "    StructField(\"PassengerId\",IntegerType(),True),\n",
    "    StructField(\"Pclass\",StringType(),True),\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sex\",StringType(),True),\n",
    "    StructField(\"Age\",DoubleType(),True),\n",
    "    StructField(\"SibSp\",DoubleType(),True),\n",
    "    StructField(\"Parch\",DoubleType(),True),\n",
    "    StructField(\"Ticket\",StringType(),True),\n",
    "    StructField(\"Fare\",DoubleType(),True),\n",
    "    StructField(\"Cabin\",StringType(),True),\n",
    "    StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "\n",
    "\n",
    "df_train,df_test= create_dataframes('./data',schema_train=schema,schema_test=schema_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine train and test\n",
    "df_train = df_train.withColumn('Mark',lit('train'))\n",
    "df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "#drop survived to append train/test for cleaning\n",
    "df = df_train.drop('Survived')\n",
    "df = df.unionAll(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for PassengerId : 0\n",
      "Missing values for Pclass : 0\n",
      "Missing values for Name : 0\n",
      "Missing values for Sex : 0\n",
      "Missing values for Age : 263\n",
      "Missing values for SibSp : 0\n",
      "Missing values for Parch : 0\n",
      "Missing values for Ticket : 0\n",
      "Missing values for Fare : 1\n",
      "Missing values for Cabin : 1014\n",
      "Missing values for Embarked : 2\n",
      "Missing values for Mark : 0\n"
     ]
    }
   ],
   "source": [
    "#missing values by column\n",
    "for column in df.columns:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cabin has a high amount of missing values so I will remove it \n",
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill missing values with the mean\n",
    "def fill_null_with_mean(df):\n",
    "    \"\"\"\n",
    "    Replaces null numeric values with\n",
    "    mean value\n",
    "    Replaces categorical string values\n",
    "    with mode\n",
    "    input: spark dataframe\n",
    "    returns: spark dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        dtype = \"%s\" % column.dataType\n",
    "        if dtype != \"StringType\":\n",
    "            mean = df.groupBy().mean(column.name).first()[0]\n",
    "            x = x.na.fill({column.name:mean})\n",
    "        else:\n",
    "            counts = df.groupBy(column.name).count()\n",
    "            mode = counts.join(\n",
    "            counts.agg(F.max(\"count\").alias(\"max_\")),\n",
    "            col(\"count\") == col(\"max_\")\n",
    "            ).limit(1).select(column.name)\n",
    "            x = x.na.fill({column.name:mode.first()[0]})     \n",
    "    return x\n",
    "\n",
    "df = fill_null_with_mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning method above could be much improved to replace missing values than with the mean but for this notebook I wanted something quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove spaces\n",
    "spaceDeleteUDF = F.udf(lambda s: s.replace(\" \", \"\"),StringType())\n",
    "df=df.withColumn('Name',spaceDeleteUDF(df[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Title cleanse \n",
    "df = df.withColumn('Surname',split('Name',',')[0])\n",
    "df = df.withColumn('name_split',F.trim(split('Name',',')[1]))\n",
    "df = df.withColumn('Title',split('name_split','\\\\.')[0])\n",
    "title_dictionary = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Sir\",\n",
    "    \"Don\":        \"Sir\",\n",
    "    \"Sir\" :       \"Sir\",\n",
    "    \"Dr\":         \"Mr\",\n",
    "    \"Rev\":        \"Mr\",\n",
    "    \"theCountess\":\"Lady\",\n",
    "    \"Dona\":       \"Lady\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Lady\"\n",
    "}\n",
    "\n",
    "#x = df['Title'].map(Title_Dictionary)\n",
    "mapping_expr = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "\n",
    "df = df.withColumn(\"Title\", mapping_expr.getItem(col(\"Title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create binary column 'Mother'\n",
    "df = df.withColumn('Mother',when((df['Sex'] =='female')&\n",
    "                                (df['Age'] > 18)&\n",
    "                                (df['Parch'] > 0)\n",
    "                                 ,'True').otherwise('False'))\n",
    "\n",
    "#create a family size column\n",
    "df = df.withColumn('Family_size',(df['SibSp'] + df['Parch'] + 1))\n",
    "\n",
    "# create a family id column\n",
    "df = df.withColumn('Family_id',when(df['Family_size']>2,\n",
    "                                   (concat(df['Surname'],\n",
    "                                    df['Family_size']))).otherwise('None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for Title : 0\n",
      "Missing values for Mother : 0\n",
      "Missing values for Family_size : 0\n",
      "Missing values for Family_id : 0\n",
      "Missing values for Surname : 0\n"
     ]
    }
   ],
   "source": [
    "for column in ['Title','Mother','Family_size','Family_id','Surname']:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns \n",
    "df = df.drop('PassengerId','Ticket','Surname','Name','name_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_on_column_types(df):\n",
    "    \"\"\"\n",
    "    Create array of numeric and string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    for col in df.schema.fields:\n",
    "        x = \"%s\" % col.dataType\n",
    "        if x == \"StringType\":\n",
    "            categorical.append(col.name)\n",
    "        else:\n",
    "            numeric.append(col.name)\n",
    "            \n",
    "            \n",
    "    return categorical,numeric\n",
    "\n",
    "categorical,numeric = split_on_column_types(train)\n",
    "#indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "#encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    \"\"\"\n",
    "    Convert dataframe into a features\n",
    "    \n",
    "    \"\"\"\n",
    "    x = df.cache()\n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    \n",
    "    for column in x.schema.fields:\n",
    "        cType = \"%s\" % column.dataType\n",
    "        if cType == \"StringType\":\n",
    "            categorical.append(column.name)\n",
    "        else:\n",
    "            numeric.append(column.name)\n",
    "            \n",
    "    \n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "    #encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]\n",
    "    index_categorical = [column + \"_index\" for column in categorical]\n",
    "    all_columns = index_categorical + numeric\n",
    "    assembler = VectorAssembler(inputCols=all_columns,outputCol='features')\n",
    "    #assembler is added to list with square brackets\n",
    "    stages = indexers + [assembler]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    x = pipeline.fit(x).transform(x)\n",
    "    return x\n",
    "\n",
    "df = convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survived', 'row_index']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split back into train and test \n",
    "train = df.where(df['Mark']=='train')\n",
    "test  = df.where(df['Mark']=='test')\n",
    "\n",
    "#append 'Survived' back on training data\n",
    "# since there is no common column between these two dataframes add row_index so that it can be joined\n",
    "train=train.withColumn('row_index', F.monotonically_increasing_id())\n",
    "survived = df_train.select('Survived')\n",
    "survived = StringIndexer(inputCol='Survived',outputCol='Survived_index').fit(survived).transform(survived)\n",
    "survived = survived.withColumn('row_index', F.monotonically_increasing_id())\n",
    "train = train.join(survived, on=[\"row_index\"]).sort(\"row_index\").drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked',\n",
       " 'Mark',\n",
       " 'Title',\n",
       " 'Mother',\n",
       " 'Family_size',\n",
       " 'Family_id',\n",
       " 'Pclass_index',\n",
       " 'Sex_index',\n",
       " 'Embarked_index',\n",
       " 'Mark_index',\n",
       " 'Title_index',\n",
       " 'Mother_index',\n",
       " 'Family_id_index',\n",
       " 'features',\n",
       " 'Survived',\n",
       " 'Survived_index']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Survived_index\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=100,\n",
    "                            maxBins=107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol='prediction',\n",
    "                               outputCol='predictedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassificationModel' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-7e6abfd875b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassificationModel' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helpmeplease = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pclass: string, Sex: string, Age: double, SibSp: double, Parch: double, Fare: double, Embarked: string, Mark: string, Title: string, Mother: string, Family_size: double, Family_id: string, Pclass_index: double, Sex_index: double, Embarked_index: double, Mark_index: double, Title_index: double, Mother_index: double, Family_id_index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpmeplease.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='prediction')\n",
    "pred = helpmeplease.transform(test)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #scale numeric columns\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n",
    "#                          ,withStd=False,withMean=False\n",
    "#                          ).fit(df) for column in numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
