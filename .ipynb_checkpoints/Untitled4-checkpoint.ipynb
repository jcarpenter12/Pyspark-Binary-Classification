{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import time\n",
    "import os.path\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString, VectorIndexer, VectorAssembler, OneHotEncoder, SQLTransformer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import StringType,StructField,IntegerType,DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat,translate,lit,col,isnan,count,when,split,explode,ltrim,create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Classifier, master=local[*]) created by __init__ at <ipython-input-26-8f4850d1539f>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-8f4850d1539f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#initialise spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Classifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 299\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Classifier, master=local[*]) created by __init__ at <ipython-input-26-8f4850d1539f>:3 "
     ]
    }
   ],
   "source": [
    "#initialise spark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName='Classifier')\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(directory,schema_train=None,schema_test=None):\n",
    "    \"\"\"\n",
    "    Creates dataframes from directory\n",
    "    Must be named 'train' or 'test'. \n",
    "    Returns only train if test N/A\n",
    "    \n",
    "    Inputs: String, schema defaults to false\n",
    "    and will infer from input .csv else will apply\n",
    "    specified schema/schemas\n",
    "    \n",
    "    Returns: Dataframes/Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    inferSchema = True if schema==None else False\n",
    "    schema_test = schema_train if schema_test==None else schema_test\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        train = directory+\"/train.csv\"\n",
    "        if os.path.exists(train):\n",
    "            df_train = sql.read.csv(train, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema)\n",
    "        else:\n",
    "            raise ValueError(\"train.csv not found in %s\" % directory)\n",
    "        \n",
    "        test = directory+\"/test.csv\"\n",
    "        if os.path.exists(test):\n",
    "            df_test = sql.read.csv(test, \n",
    "                         header = True,\n",
    "                         inferSchema = inferSchema,\n",
    "                         schema=schema_test)\n",
    "            \n",
    "            return df_train,df_test\n",
    "        \n",
    "        return df_train\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"%s does not exist\" % directory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify schema\n",
    "schema= StructType([\n",
    "    StructField(\"PassengerId\",IntegerType(),True),\n",
    "    StructField(\"Survived\",StringType(),True),\n",
    "    StructField(\"Pclass\",StringType(),True),\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sex\",StringType(),True),\n",
    "    StructField(\"Age\",DoubleType(),True),\n",
    "    StructField(\"SibSp\",DoubleType(),True),\n",
    "    StructField(\"Parch\",DoubleType(),True),\n",
    "    StructField(\"Ticket\",StringType(),True),\n",
    "    StructField(\"Fare\",DoubleType(),True),\n",
    "    StructField(\"Cabin\",StringType(),True),\n",
    "    StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "schema_test= StructType([\n",
    "    StructField(\"PassengerId\",IntegerType(),True),\n",
    "    StructField(\"Pclass\",StringType(),True),\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sex\",StringType(),True),\n",
    "    StructField(\"Age\",DoubleType(),True),\n",
    "    StructField(\"SibSp\",DoubleType(),True),\n",
    "    StructField(\"Parch\",DoubleType(),True),\n",
    "    StructField(\"Ticket\",StringType(),True),\n",
    "    StructField(\"Fare\",DoubleType(),True),\n",
    "    StructField(\"Cabin\",StringType(),True),\n",
    "    StructField(\"Embarked\",StringType(),True)])\n",
    "\n",
    "\n",
    "\n",
    "df_train,df_test= create_dataframes('./data',schema_train=schema,schema_test=schema_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(PassengerId,IntegerType,true), StructField(Survived,StringType,true), StructField(Pclass,StringType,true), StructField(Name,StringType,true), StructField(Sex,StringType,true), StructField(Age,DoubleType,true), StructField(SibSp,DoubleType,true), StructField(Parch,DoubleType,true), StructField(Ticket,StringType,true), StructField(Fare,DoubleType,true), StructField(Cabin,StringType,true), StructField(Embarked,StringType,true)]\n",
      "'''''''''''''''''''\n",
      "[StructField(PassengerId,IntegerType,true), StructField(Pclass,StringType,true), StructField(Name,StringType,true), StructField(Sex,StringType,true), StructField(Age,DoubleType,true), StructField(SibSp,DoubleType,true), StructField(Parch,DoubleType,true), StructField(Ticket,StringType,true), StructField(Fare,DoubleType,true), StructField(Cabin,StringType,true), StructField(Embarked,StringType,true)]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.schema.fields)\n",
    "\n",
    "print(\"'''''''''''''''''''\")\n",
    "print(df_test.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|  1.0|  0.0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|  1.0|  0.0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|  0.0|  0.0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|  1.0|  0.0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|  0.0|  0.0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|  0.0|  0.0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|  0.0|  0.0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|  3.0|  1.0|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|  0.0|  2.0|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|  1.0|  0.0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|  1.0|  1.0|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|  0.0|  0.0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|  0.0|  0.0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|  1.0|  5.0|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|  0.0|  0.0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|  0.0|  0.0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|  4.0|  1.0|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|  0.0|  0.0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|  1.0|  0.0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|  0.0|  0.0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Union can only be performed on tables with the same number of columns, but the first table has 12 columns and the second table has 13 columns;;\\n'Union\\n:- Project [PassengerId#95275, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, Mark#95325]\\n:  +- Project [PassengerId#95275, Survived#95276, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, train AS Mark#95325]\\n:     +- Relation[PassengerId#95275,Survived#95276,Pclass#95277,Name#95278,Sex#95279,Age#95280,SibSp#95281,Parch#95282,Ticket#95283,Fare#95284,Cabin#95285,Embarked#95286] csv\\n+- Project [PassengerId#95300, Survived#95301, Pclass#95302, Name#95303, Sex#95304, Age#95305, SibSp#95306, Parch#95307, Ticket#95308, Fare#95309, Cabin#95310, Embarked#95311, test AS Mark#95340]\\n   +- Relation[PassengerId#95300,Survived#95301,Pclass#95302,Name#95303,Sex#95304,Age#95305,SibSp#95306,Parch#95307,Ticket#95308,Fare#95309,Cabin#95310,Embarked#95311] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o14262.union.\n: org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the same number of columns, but the first table has 12 columns and the second table has 13 columns;;\n'Union\n:- Project [PassengerId#95275, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, Mark#95325]\n:  +- Project [PassengerId#95275, Survived#95276, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, train AS Mark#95325]\n:     +- Relation[PassengerId#95275,Survived#95276,Pclass#95277,Name#95278,Sex#95279,Age#95280,SibSp#95281,Parch#95282,Ticket#95283,Fare#95284,Cabin#95285,Embarked#95286] csv\n+- Project [PassengerId#95300, Survived#95301, Pclass#95302, Name#95303, Sex#95304, Age#95305, SibSp#95306, Parch#95307, Ticket#95308, Fare#95309, Cabin#95310, Embarked#95311, test AS Mark#95340]\n   +- Relation[PassengerId#95300,Survived#95301,Pclass#95302,Name#95303,Sex#95304,Age#95305,SibSp#95306,Parch#95307,Ticket#95308,Fare#95309,Cabin#95310,Embarked#95311] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$13.apply(CheckAnalysis.scala:318)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$13.apply(CheckAnalysis.scala:315)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:315)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withSetOperator(Dataset.scala:2884)\r\n\tat org.apache.spark.sql.Dataset.union(Dataset.scala:1656)\r\n\tat sun.reflect.GeneratedMethodAccessor377.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-521-1abeff505d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#drop survived to append train/test for cleaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munionAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36munionAll\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mnote\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDeprecated\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0munion\u001b[0m \u001b[0minstead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \"\"\"\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36munion\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \"\"\"\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jcarpent\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"Union can only be performed on tables with the same number of columns, but the first table has 12 columns and the second table has 13 columns;;\\n'Union\\n:- Project [PassengerId#95275, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, Mark#95325]\\n:  +- Project [PassengerId#95275, Survived#95276, Pclass#95277, Name#95278, Sex#95279, Age#95280, SibSp#95281, Parch#95282, Ticket#95283, Fare#95284, Cabin#95285, Embarked#95286, train AS Mark#95325]\\n:     +- Relation[PassengerId#95275,Survived#95276,Pclass#95277,Name#95278,Sex#95279,Age#95280,SibSp#95281,Parch#95282,Ticket#95283,Fare#95284,Cabin#95285,Embarked#95286] csv\\n+- Project [PassengerId#95300, Survived#95301, Pclass#95302, Name#95303, Sex#95304, Age#95305, SibSp#95306, Parch#95307, Ticket#95308, Fare#95309, Cabin#95310, Embarked#95311, test AS Mark#95340]\\n   +- Relation[PassengerId#95300,Survived#95301,Pclass#95302,Name#95303,Sex#95304,Age#95305,SibSp#95306,Parch#95307,Ticket#95308,Fare#95309,Cabin#95310,Embarked#95311] csv\\n\""
     ]
    }
   ],
   "source": [
    "# combine train and test\n",
    "df_train = df_train.withColumn('Mark',lit('train'))\n",
    "df_test  = df_test.withColumn('Mark',lit('test'))\n",
    "#drop survived to append train/test for cleaning\n",
    "df = df_train.drop('Survived')\n",
    "df = df.unionAll(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(PassengerId,IntegerType,true),\n",
       " StructField(Survived,IntegerType,true),\n",
       " StructField(Pclass,IntegerType,true),\n",
       " StructField(Name,StringType,true),\n",
       " StructField(Sex,StringType,true),\n",
       " StructField(Age,DoubleType,true),\n",
       " StructField(SibSp,IntegerType,true),\n",
       " StructField(Parch,IntegerType,true),\n",
       " StructField(Ticket,StringType,true),\n",
       " StructField(Fare,DoubleType,true),\n",
       " StructField(Cabin,StringType,true),\n",
       " StructField(Embarked,StringType,true),\n",
       " StructField(Mark,StringType,false)]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for PassengerId : 0\n",
      "Missing values for Pclass : 0\n",
      "Missing values for Name : 0\n",
      "Missing values for Sex : 0\n",
      "Missing values for Age : 263\n",
      "Missing values for SibSp : 0\n",
      "Missing values for Parch : 0\n",
      "Missing values for Ticket : 0\n",
      "Missing values for Fare : 1\n",
      "Missing values for Cabin : 1014\n",
      "Missing values for Embarked : 2\n",
      "Missing values for Mark : 0\n"
     ]
    }
   ],
   "source": [
    "#missing values by column\n",
    "for column in df.columns:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cabin has a high amount of missing values so I will remove it \n",
    "df = df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values with the mean\n",
    "def fill_null_with_mean(df):\n",
    "    \"\"\"\n",
    "    Replaces null numeric values with\n",
    "    mean value\n",
    "    Replaces categorical string values\n",
    "    with mode\n",
    "    input: spark dataframe\n",
    "    returns: spark dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.cache()\n",
    "    \n",
    "    for column in df.schema.fields:\n",
    "        dtype = \"%s\" % column.dataType\n",
    "        if dtype != \"StringType\":\n",
    "            mean = df.groupBy().mean(column.name).first()[0]\n",
    "            x = x.na.fill({column.name:mean})\n",
    "        else:\n",
    "            counts = df.groupBy(column.name).count()\n",
    "            mode = counts.join(\n",
    "            counts.agg(F.max(\"count\").alias(\"max_\")),\n",
    "            col(\"count\") == col(\"max_\")\n",
    "            ).limit(1).select(column.name)\n",
    "            x = x.na.fill({column.name:mode.first()[0]})     \n",
    "    return x\n",
    "\n",
    "df = fill_null_with_mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning method above could be much improved to replace missing values than with the mean but for this notebook I wanted something quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove spaces\n",
    "spaceDeleteUDF = F.udf(lambda s: s.replace(\" \", \"\"),StringType())\n",
    "df=df.withColumn('Name',spaceDeleteUDF(df[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title cleanse \n",
    "df = df.withColumn('Surname',split('Name',',')[0])\n",
    "df = df.withColumn('name_split',F.trim(split('Name',',')[1]))\n",
    "df = df.withColumn('Title',split('name_split','\\\\.')[0])\n",
    "title_dictionary = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Sir\",\n",
    "    \"Don\":        \"Sir\",\n",
    "    \"Sir\" :       \"Sir\",\n",
    "    \"Dr\":         \"Mr\",\n",
    "    \"Rev\":        \"Mr\",\n",
    "    \"theCountess\":\"Lady\",\n",
    "    \"Dona\":       \"Lady\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Lady\"\n",
    "}\n",
    "\n",
    "#x = df['Title'].map(Title_Dictionary)\n",
    "mapping_expr = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "\n",
    "df = df.withColumn(\"Title\", mapping_expr.getItem(col(\"Title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary column 'Mother'\n",
    "df = df.withColumn('Mother',when((df['Sex'] =='female')&\n",
    "                                (df['Age'] > 18)&\n",
    "                                (df['Parch'] > 0)\n",
    "                                 ,'True').otherwise('False'))\n",
    "\n",
    "#create a family size column\n",
    "df = df.withColumn('Family_size',(df['SibSp'] + df['Parch'] + 1))\n",
    "\n",
    "# create a family id column\n",
    "df = df.withColumn('Family_id',when(df['Family_size']>2,\n",
    "                                   (concat(df['Surname'],\n",
    "                                    df['Family_size']))).otherwise('None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for Title : 0\n",
      "Missing values for Mother : 0\n",
      "Missing values for Family_size : 0\n",
      "Missing values for Family_id : 0\n",
      "Missing values for Surname : 0\n"
     ]
    }
   ],
   "source": [
    "for column in ['Title','Mother','Family_size','Family_id','Surname']:\n",
    "    missing = df.where(df[column].isNull()).count()\n",
    "    print(\"Missing values for %s : %s\" % (column,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns \n",
    "df = df.drop('PassengerId','Ticket','Surname','Name','name_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split back into train and test \n",
    "train = df.where(df['Mark']=='train')\n",
    "test  = df.where(df['Mark']=='test')\n",
    "\n",
    "#append 'Survived' back on training data\n",
    "# since there is no common column between these two dataframes add row_index so that it can be joined\n",
    "train=train.withColumn('row_index', F.monotonically_increasing_id())\n",
    "survived = df_train.select('Survived')\n",
    "survived = survived.withColumn('row_index', F.monotonically_increasing_id())\n",
    "train = train.join(survived, on=[\"row_index\"]).sort(\"row_index\").drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_column_types(df):\n",
    "    \"\"\"\n",
    "    Create array of numeric and string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    string = []\n",
    "    numeric = []\n",
    "    \n",
    "    for col in df.schema.fields:\n",
    "        x = \"%s\" % col.dataType\n",
    "        if x == \"StringType\":\n",
    "            string.append(col.name)\n",
    "        else:\n",
    "            numeric.append(col.name)\n",
    "            \n",
    "            \n",
    "    return string,numeric\n",
    "\n",
    "categorical,numeric = split_on_column_types(train)\n",
    "numeric.remove('Pclass')\n",
    "categorical.remove('Mark')\n",
    "categorical.extend(['Pclass','Survived'])\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\",outputCol=column+\"_vec\") for column in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_categorical = [x + \"_index\" for x in categorical]\n",
    "all_columns = index_categorical + numeric\n",
    "assembler = VectorAssembler(inputCols=all_columns,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Survived_index\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=100,\n",
    "                            maxBins=97\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol='prediction',\n",
    "                               outputCol='predictedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(Pclass,IntegerType,true),\n",
       " StructField(Sex,StringType,false),\n",
       " StructField(Age,DoubleType,false),\n",
       " StructField(SibSp,IntegerType,true),\n",
       " StructField(Parch,IntegerType,true),\n",
       " StructField(Fare,DoubleType,false),\n",
       " StructField(Embarked,StringType,false),\n",
       " StructField(Mark,StringType,false),\n",
       " StructField(Title,StringType,true),\n",
       " StructField(Mother,StringType,false),\n",
       " StructField(Family_size,IntegerType,true),\n",
       " StructField(Family_id,StringType,true),\n",
       " StructField(Survived,IntegerType,true),\n",
       " StructField(Sex_index,DoubleType,true),\n",
       " StructField(Embarked_index,DoubleType,true),\n",
       " StructField(Title_index,DoubleType,true),\n",
       " StructField(Mother_index,DoubleType,true),\n",
       " StructField(Family_id_index,DoubleType,true),\n",
       " StructField(Pclass_index,DoubleType,true),\n",
       " StructField(Survived_index,DoubleType,true),\n",
       " StructField(features,VectorUDT,true),\n",
       " StructField(rawPrediction,VectorUDT,true),\n",
       " StructField(probability,VectorUDT,true),\n",
       " StructField(prediction,DoubleType,true)]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = indexers + [assembler,rf]\n",
    "pipeline = Pipeline(stages = stages)\n",
    "df_indexed = pipeline.fit(train).transform(train)\n",
    "df_indexed.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scale numeric columns\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scalers = [StandardScaler(inputCol=column, outputCol=column+\"_index\"\n",
    "#                          ,withStd=False,withMean=False\n",
    "#                          ).fit(df) for column in numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
